{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using mljar - Automl\n",
    "\n",
    "Links:\n",
    "\n",
    "- https://github.com/mljar/mljar-supervised\n",
    "- https://supervised.mljar.com/\n",
    "- https://github.com/mljar/mljar-supervised/blob/ede835a4f6d2fa478477b24d2728b3dd97f5351a/supervised/automl.py#L15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def make_noise():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/pica/nas_pica/Data/numerai/'\n",
    "\n",
    "from numebot_private.round_manager_extended import RoundManagerExtended\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numebot.secret import PUBLIC_ID, SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_folder = '/home/pica/nas_pica/Data/numerai/'\n",
    "\n",
    "from numebot.data.data_constants import NC\n",
    "from numebot.secret import PUBLIC_ID, SECRET_KEY\n",
    "\n",
    "from numebot_private.round_manager_extended import RoundManagerExtended\n",
    "\n",
    "rm = RoundManagerExtended(data_folder, \n",
    "                          public_id=PUBLIC_ID, \n",
    "                          secret_key=SECRET_KEY, \n",
    "                          save_memory=False,\n",
    "                          nrows=10000, testing=True\n",
    "                         )\n",
    "\n",
    "# Get list of models with their model file\n",
    "rm.models_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML \n",
    "feature_cols = [f for f in rm.data.train.columns if f.startswith(\"feature\")]\n",
    "    \n",
    "X_train = rm.data.train[feature_cols]\n",
    "y_train = rm.data.train['target']#.astype(float).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = rm.data.val[feature_cols]\n",
    "y_val = rm.data.val[NC.target]\n",
    "X_test = rm.data.test[feature_cols]\n",
    "y_test = rm.data.test[NC.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "automl_results_path = '/home/pica/nas_pica/Data/numerai/models/sandbox/automl_test'\n",
    "\n",
    "if Path(automl_results_path).exists():\n",
    "    shutil.rmtree(automl_results_path)\n",
    "\n",
    "automl = AutoML(\n",
    "    results_path=automl_results_path,\n",
    "    mode=\"Perform\", \n",
    "    total_time_limit=60,#3600*5, \n",
    "    ml_task='regression',    \n",
    "    eval_metric='spearman',\n",
    ")\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "y_test[:5]\n",
    "\n",
    "pd.Series(y_test).hist()\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_predictions = automl.predict(X_val)\n",
    "print(\"Test MSE:\", mean_squared_error(y_val, val_predictions))\n",
    "# compute the MSE on test data\n",
    "test_predictions = automl.predict(X_test)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "predictions = pd.Series(predictions, index=rm.data.tournament.index)\n",
    "predictions.head()\n",
    "predictions.hist()\n",
    "\n",
    "predictions = pd.DataFrame(predictions).rename({0: 'prediction'}, axis=1)\n",
    "\n",
    "predictions.to_csv('auto_ml_submission.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).hist()\n",
    "predictions.hist()\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the MSE on train data\n",
    "predictions_train = automl.predict(X_train)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, predictions_train))\n",
    "\n",
    "predictions_train = pd.Series(predictions_train, index=rm.data.train.index)\n",
    "predictions_train.head()\n",
    "predictions_train.hist()\n",
    "\n",
    "pd.Series(y_train).hist()\n",
    "predictions_train.hist()\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Perform\", \n",
    "    total_time_limit=3600*5, \n",
    "    ml_task='regression',    \n",
    "    eval_metric='rmse',\n",
    ")\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get preds for a model\n",
    "output = model.predict(rm.data.tournament)\n",
    "output.shape\n",
    "output.head()\n",
    "\n",
    "# Get performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for each model\n",
    "rm.generate_predictions_for_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit predictions (test with rpica_test_3)\n",
    "# Can I check if I submitted? (for example requesting the scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_=[print(attr) for attr in dir(napi) if not attr.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get competitions\n",
    "all_competitions = napi.get_competitions()\n",
    "all_competitions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get leaderboard for the current round\n",
    "leaderboard = napi.get_leaderboard(limit=10000)\n",
    "len(leaderboard)\n",
    "leaderboard_dict = {competitor['username']:competitor for competitor in leaderboard}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_dict['rpica']\n",
    "leaderboard_dict['rpica_test_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a new round has started\n",
    "if napi.check_new_round():\n",
    "    print(\"new round has started wihtin the last 24hours!\")\n",
    "else:\n",
    "    print(\"no new round within the last 24 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide api tokens\n",
    "from numebot.secret import PUBLIC_KEY, PRIVATE_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi = numerapi.NumerAPI(PUBLIC_KEY, PRIVATE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = napi.get_models()\n",
    "\n",
    "# upload predictions\n",
    "#submission_id = napi.upload_predictions(\"preds.csv\", tournament=1)\n",
    "# check submission status\n",
    "napi.submission_status(model_id=models_dict['rpica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_FOLDER = Path('/home/pica/hdd/nas/Data/numerai/21-03-14 weekly/')\n",
    "DATA_FOLDER = Path('/home/pica/hdd/nas/Data/numerai/numerai_dataset_258/')\n",
    "OUTPUT_FOLDER = Path('/home/pica/hdd/nas/Data/numerai/output/')\n",
    "\n",
    "# train data contains features and targets\n",
    "training_data = pd.read_csv(DATA_FOLDER/\"numerai_training_data.csv\").set_index(\"id\")\n",
    "\n",
    "# tournament data contains features only\n",
    "tournament_data = pd.read_csv(DATA_FOLDER/\"numerai_tournament_data.csv\").set_index(\"id\")\n",
    "feature_names = [f for f in training_data.columns if \"feature\" in f]\n",
    "\n",
    "live_data = tournament_data[tournament_data['data_type'] == 'live']\n",
    "tournament_data = tournament_data[tournament_data['data_type'] != 'live']\n",
    "\n",
    "training_data['era'] = training_data['era'].str.lstrip('era').astype(int)\n",
    "tournament_data['era'] = tournament_data['era'].str.lstrip('era').astype(int)\n",
    "live_data['era'] = live_data['era'].str.lstrip('era')\n",
    "\n",
    "training_data.shape\n",
    "tournament_data.shape\n",
    "live_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T05:53:42.006522Z",
     "start_time": "2021-04-05T05:53:41.175801Z"
    }
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.info()\n",
    "live_data.info()\n",
    "tournament_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset, set_name in zip([training_data, tournament_data, live_data], ['train', 'tournament', 'live']):\n",
    "    print(f'Info about {set_name}: shape {dataset.shape}')\n",
    "    #dataset[[col for col in dataset.columns if 'feature' not in col]].head(2)\n",
    "    dataset.groupby('data_type')['era'].agg(['count','min','max', pd.Series.nunique, lambda x: sorted(list(np.unique(x)))])\n",
    "    \n",
    "\n",
    "train_era = training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with feature neutralization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(df, target=\"prediction_kazutsugi\", by=None, proportion=1.0):\n",
    "    if by is None:\n",
    "        by = [x for x in df.columns if x.startswith('feature')]\n",
    "\n",
    "    scores = df[target]\n",
    "    exposures = df[by].values\n",
    "\n",
    "    # constant column to make sure the series is completely neutral to exposures\n",
    "    exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n",
    "\n",
    "    scores -= proportion * (exposures @ (np.linalg.pinv(exposures) @ scores.values))\n",
    "    return scores / scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('/home/pica/hdd/nas/Data/numerai/numerai_dataset_258/')\n",
    "OUTPUT_PATH = DATA_FOLDER/'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.read_csv(OUTPUT_PATH).set_index(\"id\")\n",
    "outputs.shape\n",
    "outputs.head(2)\n",
    "len(outputs) - outputs.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_data = pd.read_csv(DATA_FOLDER/\"numerai_tournament_data.csv\").set_index(\"id\")\n",
    "tournament_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_data['target'] = outputs\n",
    "tournament_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralized = neutralize(tournament_data, target='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralized = pd.DataFrame(neutralized)\n",
    "neutralized.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "neutralized.rename({'target': 'prediction'}, axis=1, inplace=True)\n",
    "neutralized.head()\n",
    "neutralized_scaled = neutralized.copy()\n",
    "neutralized_scaled[['prediction']] = scaler.fit_transform(neutralized[['prediction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralized_scaled.describe().loc[['min', 'max'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralized_scaled.to_csv(DATA_FOLDER/\"submission_neutralized.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train a model to make predictions on tournament data\n",
    "model = XGBRegressor(max_depth=5, learning_rate=0.01, \\\n",
    "                     n_estimators=2000, colsample_bytree=0.1, n_jobs=-1)\n",
    "model.fit(training_data[feature_names], training_data[\"target\"])\n",
    "\n",
    "# submit predictions to numer.ai\n",
    "predictions = model.predict(tournament_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({'predictions': predictions}, index=tournament_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(predictions).shape\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "pd.Series(predictions).to_csv(OUTPUT_FOLDER/\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for data in [training_data, tournament_data]:\n",
    "    print(data.shape)\n",
    "    print(data[['era']].nunique())\n",
    "    print(data[['era']].max())\n",
    "    print(data.index.nunique())\n",
    "    data.head()\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_data.head()\n",
    "tournament_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tournament_data.shape\n",
    "tournament_data.head()\n",
    "\n",
    "for era in tournament_data['era'].unique():\n",
    "    ids_in_era = tournament_data[tournament_data['era'] == era].index\n",
    "    era_preds = predictions[predictions.index.isin(ids_in_era)]\n",
    "    \n",
    "    if era_preds['predictions'].nunique() != len(era_preds):\n",
    "        print(f'Repeated values in {era}: len {len(era_preds)}, unique values: {era_preds[\"predictions\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute the MSE on test data\n",
    "predictions = automl.predict(X_test)\n",
    "valid = ~np.isnan(y_test)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test[valid], predictions[valid]))\n",
    "\n",
    "predictions = pd.Series(predictions, index=rm.data.tournament.index)\n",
    "predictions.head()\n",
    "predictions.hist()\n",
    "\n",
    "predictions = pd.DataFrame(predictions).rename({0: 'prediction'}, axis=1)\n",
    "#predictions.to_csv('auto_ml_submission.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y_test).hist()\n",
    "predictions.hist()\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the MSE on train data\n",
    "predictions_train = automl.predict(X_train)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, predictions_train))\n",
    "\n",
    "predictions_train = pd.Series(predictions_train, index=rm.data.train.index)\n",
    "predictions_train.head()\n",
    "predictions_train.hist()\n",
    "\n",
    "pd.Series(y_train).hist()\n",
    "predictions_train.hist()\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Perform\", \n",
    "    total_time_limit=3600*10, \n",
    "    ml_task='regression',    \n",
    "    eval_metric='r2',\n",
    ")\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MSE on test data\n",
    "predictions = automl.predict(X_test)\n",
    "valid = ~np.isnan(y_test)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test[valid], predictions[valid]))\n",
    "\n",
    "predictions = pd.Series(predictions, index=rm.data.tournament.index)\n",
    "predictions.head()\n",
    "predictions.hist()\n",
    "\n",
    "predictions = pd.DataFrame(predictions).rename({0: 'prediction'}, axis=1)\n",
    "#predictions.to_csv('auto_ml_submission.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).hist()\n",
    "predictions.hist()\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the MSE on train data\n",
    "predictions_train = automl.predict(X_train)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, predictions_train))\n",
    "\n",
    "predictions_train = pd.Series(predictions_train, index=rm.data.train.index)\n",
    "predictions_train.head()\n",
    "predictions_train.hist()\n",
    "\n",
    "pd.Series(y_train).hist()\n",
    "predictions_train.hist()\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rmse - with noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_noise = rm.data.noisy_training(5)\n",
    "print(train_with_noise.shape)\n",
    "for col in feature_cols:\n",
    "    train_with_noise[col] = train_with_noise[col].astype(float)\n",
    "    \n",
    "X_train = train_with_noise[feature_cols]\n",
    "y_train = train_with_noise['target'].astype(float).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Perform\", \n",
    "    total_time_limit=3600*6, \n",
    "    ml_task='regression',    \n",
    "    eval_metric='rmse',\n",
    "    n_jobs=22,\n",
    ")\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MSE on test data\n",
    "predictions = automl.predict(X_test)\n",
    "valid = ~np.isnan(y_test)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test[valid], predictions[valid]))\n",
    "\n",
    "predictions = pd.Series(predictions, index=rm.data.tournament.index)\n",
    "predictions.head()\n",
    "predictions.hist()\n",
    "\n",
    "predictions = pd.DataFrame(predictions).rename({0: 'prediction'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('auto_ml_submission_with_noise.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "def make_noise():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "make_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).hist()\n",
    "predictions.hist()\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MSE on train data\n",
    "predictions_train = automl.predict(X_train)\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_train = pd.Series(predictions_train, index=train_with_noise.index)\n",
    "predictions_train.head()\n",
    "predictions_train.hist()\n",
    "\n",
    "pd.Series(y_train).hist()\n",
    "predictions_train.hist()\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carefull i run and remove a create - noisy df cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_noise = rm.data.noisy_training(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
